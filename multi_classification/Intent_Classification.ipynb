{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import random\n",
    "import nltk\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>locale</th>\n",
       "      <th>i_name</th>\n",
       "      <th>example</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>en</td>\n",
       "      <td>greetings</td>\n",
       "      <td>hi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>en</td>\n",
       "      <td>greetings</td>\n",
       "      <td>hello</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>en</td>\n",
       "      <td>greetings</td>\n",
       "      <td>hey</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>en</td>\n",
       "      <td>order</td>\n",
       "      <td>I am hungry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>en</td>\n",
       "      <td>order</td>\n",
       "      <td>I'd like a pizza</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>en</td>\n",
       "      <td>order</td>\n",
       "      <td>I want to order a pizza</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>en</td>\n",
       "      <td>order</td>\n",
       "      <td>I want to order a large pizza</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>en</td>\n",
       "      <td>order</td>\n",
       "      <td>Can I get a pizza margherita small please</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>en</td>\n",
       "      <td>order</td>\n",
       "      <td>za</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>en</td>\n",
       "      <td>order</td>\n",
       "      <td>order</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>en</td>\n",
       "      <td>help</td>\n",
       "      <td>help</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>en</td>\n",
       "      <td>help</td>\n",
       "      <td>help please</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>en</td>\n",
       "      <td>help</td>\n",
       "      <td>whats can i do</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>en</td>\n",
       "      <td>help</td>\n",
       "      <td>i dont understand</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>en</td>\n",
       "      <td>exit</td>\n",
       "      <td>get me out of here</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>en</td>\n",
       "      <td>exit</td>\n",
       "      <td>get me out</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>en</td>\n",
       "      <td>exit</td>\n",
       "      <td>cancel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>en</td>\n",
       "      <td>exit</td>\n",
       "      <td>exit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>en</td>\n",
       "      <td>order</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>en</td>\n",
       "      <td>order</td>\n",
       "      <td>i am done</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>en</td>\n",
       "      <td>reset</td>\n",
       "      <td>begin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>en</td>\n",
       "      <td>reset</td>\n",
       "      <td>reset</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>en</td>\n",
       "      <td>reset</td>\n",
       "      <td>delete the inputs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>en</td>\n",
       "      <td>reset</td>\n",
       "      <td>start again</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   locale     i_name                                    example\n",
       "0      en  greetings                                         hi\n",
       "1      en  greetings                                      hello\n",
       "2      en  greetings                                        hey\n",
       "3      en      order                                I am hungry\n",
       "4      en      order                           I'd like a pizza\n",
       "5      en      order                    I want to order a pizza\n",
       "6      en      order              I want to order a large pizza\n",
       "7      en      order  Can I get a pizza margherita small please\n",
       "8      en      order                                         za\n",
       "9      en      order                                      order\n",
       "10     en       help                                       help\n",
       "11     en       help                                help please\n",
       "12     en       help                             whats can i do\n",
       "13     en       help                          i dont understand\n",
       "14     en       exit                         get me out of here\n",
       "15     en       exit                                 get me out\n",
       "16     en       exit                                     cancel\n",
       "17     en       exit                                       exit\n",
       "18     en      order                                         no\n",
       "19     en      order                                  i am done\n",
       "20     en      reset                                      begin\n",
       "21     en      reset                                      reset\n",
       "22     en      reset                          delete the inputs\n",
       "23     en      reset                                start again"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"text_data.csv\")\n",
    "data.head(24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>locale</th>\n",
       "      <th>i_name</th>\n",
       "      <th>example</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>en</td>\n",
       "      <td>reset</td>\n",
       "      <td>delete the inputs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>en</td>\n",
       "      <td>order</td>\n",
       "      <td>order</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>en</td>\n",
       "      <td>greetings</td>\n",
       "      <td>hey</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>en</td>\n",
       "      <td>order</td>\n",
       "      <td>I want to order a large pizza</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>en</td>\n",
       "      <td>exit</td>\n",
       "      <td>get me out of here</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>en</td>\n",
       "      <td>order</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>en</td>\n",
       "      <td>reset</td>\n",
       "      <td>begin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>en</td>\n",
       "      <td>exit</td>\n",
       "      <td>cancel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>en</td>\n",
       "      <td>greetings</td>\n",
       "      <td>hi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>en</td>\n",
       "      <td>greetings</td>\n",
       "      <td>hello</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>en</td>\n",
       "      <td>help</td>\n",
       "      <td>whats can i do</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>en</td>\n",
       "      <td>order</td>\n",
       "      <td>za</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>en</td>\n",
       "      <td>order</td>\n",
       "      <td>I am hungry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>en</td>\n",
       "      <td>order</td>\n",
       "      <td>i am done</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>en</td>\n",
       "      <td>reset</td>\n",
       "      <td>start again</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>en</td>\n",
       "      <td>exit</td>\n",
       "      <td>exit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>en</td>\n",
       "      <td>help</td>\n",
       "      <td>help</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>en</td>\n",
       "      <td>exit</td>\n",
       "      <td>get me out</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>en</td>\n",
       "      <td>order</td>\n",
       "      <td>Can I get a pizza margherita small please</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>en</td>\n",
       "      <td>reset</td>\n",
       "      <td>reset</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>en</td>\n",
       "      <td>order</td>\n",
       "      <td>I want to order a pizza</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>en</td>\n",
       "      <td>order</td>\n",
       "      <td>I'd like a pizza</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>en</td>\n",
       "      <td>help</td>\n",
       "      <td>i dont understand</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>en</td>\n",
       "      <td>help</td>\n",
       "      <td>help please</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   locale     i_name                                    example\n",
       "22     en      reset                          delete the inputs\n",
       "9      en      order                                      order\n",
       "2      en  greetings                                        hey\n",
       "6      en      order              I want to order a large pizza\n",
       "14     en       exit                         get me out of here\n",
       "18     en      order                                         no\n",
       "20     en      reset                                      begin\n",
       "16     en       exit                                     cancel\n",
       "0      en  greetings                                         hi\n",
       "1      en  greetings                                      hello\n",
       "12     en       help                             whats can i do\n",
       "8      en      order                                         za\n",
       "3      en      order                                I am hungry\n",
       "19     en      order                                  i am done\n",
       "23     en      reset                                start again\n",
       "17     en       exit                                       exit\n",
       "10     en       help                                       help\n",
       "15     en       exit                                 get me out\n",
       "7      en      order  Can I get a pizza margherita small please\n",
       "21     en      reset                                      reset\n",
       "5      en      order                    I want to order a pizza\n",
       "4      en      order                           I'd like a pizza\n",
       "13     en       help                          i dont understand\n",
       "11     en       help                                help please"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data.sample(frac=1)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "order        9\n",
       "exit         4\n",
       "reset        4\n",
       "help         4\n",
       "greetings    3\n",
       "Name: i_name, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"i_name\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "i_name_tags = ['greetings', 'order', 'help', 'exit', 'reset']\n",
    "plt.figure(figsize=(10,4))\n",
    "data.i_name.value_counts().plot(kind='bar');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_size = int(len(data) * .8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = data['example'][:train_size]\n",
    "y_train = data['i_name'][:train_size]\n",
    "\n",
    "#train labels\n",
    "labels_train = data['i_name'][:train_size]\n",
    "\n",
    "#test features\n",
    "X_test = data['example'][train_size:]\n",
    "y_test = data['i_name'][train_size:]\n",
    "\n",
    "#test labels\n",
    "labels_test = data['i_name'][train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19 5\n",
      "----------------------------\n",
      "(5,)\n"
     ]
    }
   ],
   "source": [
    "print(len(X_train), len(X_test))\n",
    "print('----------------------------')\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "layers = keras.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# vectorizer = TfidfVectorizer(stop_words=\"english\", max_features=1000, decode_error=\"ignore\")\n",
    "# vectorizer.fit(X_train)\n",
    "vocab_size = 1 #hyperparameter experiment\n",
    "tokenize = keras.preprocessing.text.Tokenizer(num_words=vocab_size, char_level=False)\n",
    "tokenize.fit_on_texts(X_train) #only fit the train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# X_train_vectorized = vectorizer.transform(X_train)\n",
    "# print(X_train_vectorized)\n",
    "\n",
    "# wide feature 1: sparse bag of words (bow) vocab_size_vector\n",
    "X_bow_train = tokenize.texts_to_matrix(X_train)\n",
    "X_bow_test = tokenize.texts_to_matrix(X_test)\n",
    "# X_bow_train\n",
    "X_bow_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5,)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21    reset\n",
      "5     order\n",
      "4     order\n",
      "13     help\n",
      "11     help\n",
      "Name: i_name, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# wide feature 2: one-hot vector of variety categories\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "# Using sklearn utility to convert label strings to numbered index\n",
    "encoder = LabelEncoder()\n",
    "print(y_test)\n",
    "encoder.fit(y_train)\n",
    "y_train = encoder.transform(y_train)\n",
    "y_test = encoder.transform(y_test)\n",
    "num_class = np.max(y_train) + 1\n",
    "\n",
    "# Converting labels to one-hot\n",
    "y_train = keras.utils.to_categorical(y_train, num_class)\n",
    "y_test = keras.utils.to_categorical(y_test, num_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_inputs = layers.Input(shape=(vocab_size))\n",
    "y_inputs = layers.Input(shape=(num_class,))\n",
    "merged_layer = layers.concatenate([bow_inputs, y_inputs])\n",
    "merged_layer = layers.Dense(256, activation='relu')(merged_layer)\n",
    "predictions = layers.Dense(1)(merged_layer)\n",
    "wide_model = keras.Model(inputs=[bow_inputs, y_inputs], outputs=predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'input_1:0' shape=(None, 1) dtype=float32>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 5)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 6)            0           input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 256)          1792        concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1)            257         dense[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 2,049\n",
      "Trainable params: 2,049\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "wide_model.compile(loss='mse', optimizer='adam', metrics=['accuracy'])\n",
    "print(wide_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_embed = tokenize.texts_to_sequences(X_train)\n",
    "test_embed = tokenize.texts_to_sequences(X_test)\n",
    "\n",
    "max_seq_length = 50\n",
    "train_embed = keras.preprocessing.sequence.pad_sequences(\n",
    "train_embed, maxlen = max_seq_length, padding=\"post\")\n",
    "test_embed = keras.preprocessing.sequence.pad_sequences(\n",
    "test_embed, maxlen = max_seq_length, padding=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         [(None, 50)]              0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, 50, 8)             8         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 400)               0         \n",
      "=================================================================\n",
      "Total params: 8\n",
      "Trainable params: 8\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "deep_inputs = layers.Input(shape=(max_seq_length,))\n",
    "embedding = layers.Embedding(vocab_size, 8, input_length=max_seq_length)(deep_inputs)\n",
    "embedding = layers.Flatten()(embedding)\n",
    "embed_out = layers.Flatten()(embedding)\n",
    "deep_model = keras.Model(inputs=deep_inputs, outputs=embed_out)\n",
    "print(deep_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "deep_model.compile(loss='mse', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 5)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            [(None, 50)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 6)            0           input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 50, 8)        8           input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 256)          1792        concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 400)          0           embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1)            257         dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 400)          0           flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 401)          0           dense_1[0][0]                    \n",
      "                                                                 flatten_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 2,057\n",
      "Trainable params: 2,057\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "merged_out = layers.concatenate([wide_model.output, deep_model.output])\n",
    "merger_out = layers.Dense(1)(merged_out)\n",
    "combined_model = keras.Model(wide_model.input + [deep_model.input], merged_out)\n",
    "print(combined_model.summary())\n",
    "\n",
    "combined_model.compile(loss='mse', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: (<class 'list'> containing values of types {\"<class 'pandas.core.series.Series'>\", \"<class 'numpy.ndarray'>\"}), <class 'NoneType'>\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input arrays should have the same number of samples as target arrays. Found 19 input samples and 5 target samples.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-0deb29bccd3c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Run training\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mcombined_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtrain_embed\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    727\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 728\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    729\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    730\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[0;32m    640\u001b[0m         \u001b[0msteps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    641\u001b[0m         \u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_split\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 642\u001b[1;33m         shuffle=shuffle)\n\u001b[0m\u001b[0;32m    643\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    644\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[0;32m   2532\u001b[0m       \u001b[1;31m# Check that all arrays have the same length.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2533\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_distribution_strategy\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2534\u001b[1;33m         \u001b[0mtraining_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck_array_lengths\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2535\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_graph_network\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_eagerly\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2536\u001b[0m           \u001b[1;31m# Additional checks to avoid users mistakenly using improper loss fns.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mcheck_array_lengths\u001b[1;34m(inputs, targets, weights)\u001b[0m\n\u001b[0;32m    675\u001b[0m                      \u001b[1;34m'the same number of samples as target arrays. '\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    676\u001b[0m                      \u001b[1;34m'Found '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset_x\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' input samples '\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 677\u001b[1;33m                      'and ' + str(list(set_y)[0]) + ' target samples.')\n\u001b[0m\u001b[0;32m    678\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset_w\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    679\u001b[0m     raise ValueError('All sample_weight arrays should have '\n",
      "\u001b[1;31mValueError\u001b[0m: Input arrays should have the same number of samples as target arrays. Found 19 input samples and 5 target samples."
     ]
    }
   ],
   "source": [
    "# Run training\n",
    "combined_model.fit([X_train] + [y_train] + [train_embed], labels_test, epochs=10, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "cls = MultinomialNB()\n",
    "# transform the list of text to tf-idf before passing it to the model\n",
    "cls.fit(vectorizer.transform(X_train), y_train)\n",
    " \n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    " \n",
    "y_pred = cls.predict(vectorizer.transform(X_test))\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.model_selection import cross_val_score\n",
    " \n",
    "# start with the classic\n",
    "# with either pure counts or tfidf features\n",
    "sgd = Pipeline([\n",
    "        (\"count vectorizer\", CountVectorizer(stop_words=\"english\", max_features=3000)),\n",
    "        (\"sgd\", SGDClassifier(loss=\"modified_huber\"))\n",
    "    ])\n",
    "sgd_tfidf = Pipeline([\n",
    "        (\"tfidf_vectorizer\", TfidfVectorizer(stop_words=\"english\", max_features=3000)),\n",
    "        (\"sgd\", SGDClassifier(loss=\"modified_huber\"))\n",
    "    ])\n",
    " \n",
    "svc = Pipeline([\n",
    "        (\"count_vectorizer\", CountVectorizer(stop_words=\"english\", max_features=3000)),\n",
    "        (\"linear svc\", SVC(kernel=\"linear\"))\n",
    "    ])\n",
    "svc_tfidf = Pipeline([\n",
    "        (\"tfidf_vectorizer\", TfidfVectorizer(stop_words=\"english\", max_features=3000)),\n",
    "        (\"linear svc\", SVC(kernel=\"linear\"))\n",
    "    ])\n",
    "   \n",
    "all_models = [\n",
    "    (\"sgd\", sgd),\n",
    "    (\"sgd_tfidf\", sgd_tfidf),\n",
    "    (\"svc\", svc),\n",
    "    (\"svc_tfidf\", svc_tfidf),\n",
    "    ]\n",
    " \n",
    "unsorted_scores = [(name, cross_val_score(model, X_train, y_train, cv=2).mean()) for name, model in all_models]\n",
    "scores = sorted(unsorted_scores, key=lambda x: -x[1])\n",
    "print(scores)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = svc_tfidf\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logreg = Pipeline([('vect', CountVectorizer()),\n",
    "                ('tfidf', TfidfTransformer()),\n",
    "                ('clf', LogisticRegression(n_jobs=1, C=1e5)),\n",
    "               ])\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "#%%time\n",
    "\n",
    "y_pred = logreg.predict(X_test)\n",
    "\n",
    "print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
    "print(classification_report(y_test, y_pred,target_names=my_tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
